{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# # Para deixar os plots brancos\n",
    "# params = {\"ytick.color\" : \"w\",\n",
    "#           \"xtick.color\" : \"w\",\n",
    "#           \"axes.titlecolor\" : \"w\",\n",
    "#           \"axes.labelcolor\" : \"w\",\n",
    "#           \"axes.edgecolor\" : \"w\"}\n",
    "# plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voltages in PU\n",
    "net_result = np.load(r'C:\\Users\\kaioh\\Documents\\Universidade\\State Estimation\\dataset\\MC_3phase_PU_std_01_samples_1000_10_01-09_55.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voltages in Volts\n",
    "net_result = np.load(r'C:\\Users\\kaioh\\Documents\\Universidade\\State Estimation\\dataset\\MC_3phase_V_std_01_samples_1000_21_12-12_26.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "voltages1 = net_result['voltages'][0]\n",
    "voltages2 = net_result['voltages'][1]\n",
    "voltages3 = net_result['voltages'][2]\n",
    "\n",
    "# 58 connections to node 1 - 50 connections to node 2 - 46 connections to node 3 \n",
    "voltages1 = voltages1[~np.isnan(voltages1)].reshape(144000,48)\n",
    "voltages2 = voltages2[~np.isnan(voltages2)].reshape(144000,50)\n",
    "voltages3 = voltages3[~np.isnan(voltages3)].reshape(144000,46)\n",
    "\n",
    "voltages1 = pd.DataFrame(voltages1)\n",
    "voltages2 = pd.DataFrame(voltages2)\n",
    "voltages3 = pd.DataFrame(voltages3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_trafo1 = pd.DataFrame(net_result['p_trafo'][0], columns = ['p_trafo1'])\n",
    "q_trafo1 = pd.DataFrame(net_result['q_trafo'][0], columns = ['q_trafo1'])\n",
    "p_trafo2 = pd.DataFrame(net_result['p_trafo'][1], columns = ['p_trafo2'])\n",
    "q_trafo2 = pd.DataFrame(net_result['q_trafo'][1], columns = ['q_trafo2'])\n",
    "p_trafo3 = pd.DataFrame(net_result['p_trafo'][2], columns = ['p_trafo3'])\n",
    "q_trafo3 = pd.DataFrame(net_result['q_trafo'][2], columns = ['q_trafo3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_result = loadmat('dataset/resultados_w1000pts.mat')\n",
    "demand_p1 = net_result['demand_p'][0]\n",
    "demand_q1 = net_result['demand_q'][0]\n",
    "demand_p2 = net_result['demand_p'][1]\n",
    "demand_q2 = net_result['demand_q'][1]\n",
    "demand_p3 = net_result['demand_p'][2]\n",
    "demand_q3 = net_result['demand_q'][2]\n",
    "\n",
    "# 51 connections to node 1 - 49 connections to node 2 - 51 connections to node 3 \n",
    "demand_p1 = demand_p1[~np.isnan(demand_p1)].reshape(144,1000,51)\n",
    "demand_q1 = demand_q1[~np.isnan(demand_q1)].reshape(144,1000,51)\n",
    "demand_p2 = demand_p2[~np.isnan(demand_p2)].reshape(144,1000,49)\n",
    "demand_q2 = demand_q2[~np.isnan(demand_q2)].reshape(144,1000,49)\n",
    "demand_p3 = demand_p3[~np.isnan(demand_p3)].reshape(144,1000,51)\n",
    "demand_q3 = demand_q3[~np.isnan(demand_q3)].reshape(144,1000,51)\n",
    "\n",
    "# take the mean\n",
    "avgP1 = demand_p1[:, :999, :].mean(axis=1)\n",
    "avgQ1 = demand_q1[:, :999, :].mean(axis=1)\n",
    "avgP2 = demand_p2[:, :999, :].mean(axis=1)\n",
    "avgQ2 = demand_q2[:, :999, :].mean(axis=1)\n",
    "avgP3 = demand_p3[:, :999, :].mean(axis=1)\n",
    "avgQ3 = demand_q3[:, :999, :].mean(axis=1)\n",
    "\n",
    "avgP1 = pd.concat([pd.DataFrame(avgP1)]*1000, ignore_index=True)\n",
    "avgQ1 = pd.concat([pd.DataFrame(avgQ1)]*1000, ignore_index=True)\n",
    "avgP2 = pd.concat([pd.DataFrame(avgP2)]*1000, ignore_index=True)\n",
    "avgQ2 = pd.concat([pd.DataFrame(avgQ2)]*1000, ignore_index=True)\n",
    "avgP3 = pd.concat([pd.DataFrame(avgP3)]*1000, ignore_index=True)\n",
    "avgQ3 = pd.concat([pd.DataFrame(avgQ3)]*1000, ignore_index=True)\n",
    "\n",
    "avgP1.columns = [f'P1_{col_name}' for col_name in avgP1.columns]\n",
    "avgP2.columns = [f'P2_{col_name}' for col_name in avgP2.columns]\n",
    "avgP3.columns = [f'P3_{col_name}' for col_name in avgP3.columns]\n",
    "avgQ1.columns = [f'Q1_{col_name}' for col_name in avgQ1.columns]\n",
    "avgQ2.columns = [f'Q2_{col_name}' for col_name in avgQ2.columns]\n",
    "avgQ3.columns = [f'Q3_{col_name}' for col_name in avgQ3.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = np.load(r'dataset\\PV_Curves_Ijui_RS_1000_Samples_15_12-16_52.npz')\n",
    "\n",
    "pv = pd.DataFrame(pv['PV_s'].flatten(), columns = ['pv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the input arrays for train and test\n",
    "## X - train set 70%\n",
    "## X_test - test set 30%\n",
    "\n",
    "X1 = pd.concat([pv, p_trafo1, avgP1, q_trafo1, avgQ1], axis=1)\n",
    "X2 = pd.concat([pv, p_trafo2, avgP2, q_trafo2, avgQ2], axis=1)\n",
    "X3 = pd.concat([pv, p_trafo3, avgP3, q_trafo3, avgQ3], axis=1)\n",
    "\n",
    "\n",
    "# # Removing features that don't change\n",
    "# features_to_drop = X.nunique()\n",
    "# features_to_drop = features_to_drop.loc[features_to_drop.values==1].index\n",
    "\n",
    "# X = X.drop(features_to_drop,axis=1)\n",
    "\n",
    "X1 = X1.iloc[21600:,:]\n",
    "X1_test = X1.iloc[:21600,:]\n",
    "X2 = X2.iloc[21600:,:]\n",
    "X2_test = X2.iloc[:21600,:]\n",
    "X3 = X3.iloc[21600:,:]\n",
    "X3_test = X3.iloc[:21600,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the output arrays for train and test, and droping the reference(trafo) buses\n",
    "## Y - train set\n",
    "## y_test - test set\n",
    "Y1 = voltages1\n",
    "y1_test = Y1.iloc[:21600,:]\n",
    "Y1 = Y1.iloc[21600:,:]\n",
    "\n",
    "Y2 = voltages2\n",
    "y2_test = Y2.iloc[:21600,:]\n",
    "Y2 = Y2.iloc[21600:,:]\n",
    "\n",
    "Y3 = voltages3\n",
    "y3_test = Y3.iloc[:21600,:]\n",
    "Y3 = Y3.iloc[21600:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 1\n",
    "\n",
    "if phase == 1:\n",
    "    X = X1\n",
    "    Y = Y1\n",
    "    X_test = X1_test\n",
    "    y_test = y1_test\n",
    "elif phase == 2:\n",
    "    X = X2\n",
    "    Y = Y2\n",
    "    X_test = X2_test\n",
    "    y_test = y2_test\n",
    "elif phase == 3:\n",
    "    X = X3\n",
    "    Y = Y3\n",
    "    X_test = X3_test\n",
    "    y_test = y3_test\n",
    "\n",
    "p_columns = [name for name in X.columns if name.startswith('P')]\n",
    "q_columns = [name for name in X.columns if name.startswith('Q')]\n",
    "p_trafo = [name for name in X.columns if name.startswith('p_')]\n",
    "q_trafo = [name for name in X.columns if name.startswith('q_')]\n",
    "pv_column = [name for name in X.columns if name.startswith('pv')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaioh\\anaconda3\\envs\\nn\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Spliting the train dataset into train and validation data\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X,Y,test_size=0.176,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaioh\\AppData\\Local\\Temp\\ipykernel_18600\\3220760517.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.loc[:, p_columns] = active_power_scaler.transform(X_test.loc[:, p_columns])\n",
      "C:\\Users\\kaioh\\AppData\\Local\\Temp\\ipykernel_18600\\3220760517.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.loc[:, pv_column] = pv_scaler.transform(X_test.loc[:, pv_column])\n",
      "C:\\Users\\kaioh\\AppData\\Local\\Temp\\ipykernel_18600\\3220760517.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.loc[:, p_trafo] = ptrafo_scaler.transform(X_test.loc[:, p_trafo])\n",
      "C:\\Users\\kaioh\\AppData\\Local\\Temp\\ipykernel_18600\\3220760517.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.loc[:, q_columns] = reactive_power_scaler.transform(X_test.loc[:, q_columns])\n",
      "C:\\Users\\kaioh\\AppData\\Local\\Temp\\ipykernel_18600\\3220760517.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.loc[:, q_trafo] = qtrafo_scaler.transform(X_test.loc[:, q_trafo])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "active_power_scaler = MinMaxScaler()\n",
    "X_train.loc[:, p_columns] = active_power_scaler.fit_transform(X_train.loc[:, p_columns])\n",
    "X_eval.loc[:, p_columns] = active_power_scaler.transform(X_eval.loc[:, p_columns])\n",
    "X_test.loc[:, p_columns] = active_power_scaler.transform(X_test.loc[:, p_columns])\n",
    "\n",
    "pv_scaler = MinMaxScaler()\n",
    "X_train.loc[:, pv_column] = pv_scaler.fit_transform(X_train.loc[:, pv_column])\n",
    "X_eval.loc[:, pv_column] = pv_scaler.transform(X_eval.loc[:, pv_column])\n",
    "X_test.loc[:, pv_column] = pv_scaler.transform(X_test.loc[:, pv_column])\n",
    "\n",
    "ptrafo_scaler = MinMaxScaler()\n",
    "X_train.loc[:, p_trafo] = ptrafo_scaler.fit_transform(X_train.loc[:, p_trafo])\n",
    "X_eval.loc[:, p_trafo] = ptrafo_scaler.transform(X_eval.loc[:, p_trafo])\n",
    "X_test.loc[:, p_trafo] = ptrafo_scaler.transform(X_test.loc[:, p_trafo])\n",
    "\n",
    "reactive_power_scaler = MinMaxScaler()\n",
    "X_train.loc[:, q_columns] = reactive_power_scaler.fit_transform(X_train.loc[:, q_columns])\n",
    "X_eval.loc[:, q_columns] = reactive_power_scaler.transform(X_eval.loc[:, q_columns])\n",
    "X_test.loc[:, q_columns] = reactive_power_scaler.transform(X_test.loc[:, q_columns])\n",
    "\n",
    "qtrafo_scaler = MinMaxScaler()\n",
    "X_train.loc[:, q_trafo] = qtrafo_scaler.fit_transform(X_train.loc[:, q_trafo])\n",
    "X_eval.loc[:, q_trafo] = qtrafo_scaler.transform(X_eval.loc[:, q_trafo])\n",
    "X_test.loc[:, q_trafo] = qtrafo_scaler.transform(X_test.loc[:, q_trafo])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import keras_tuner as kt\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        hp_units = hp.Int('units', min_value=100, max_value=300, step=100)\n",
    "        \n",
    "        hp_dropout_rate = hp.Choice('rate', values=[0.0, 0.2])\n",
    "\n",
    "        model.add(Dense(units=hp_units, activation='relu', input_shape=(X_train.shape[-1],)))\n",
    "        model.add(Dropout(rate=hp_dropout_rate))\n",
    "        model.add(Dense(units=hp_units, activation='relu'))\n",
    "        model.add(Dropout(rate=hp_dropout_rate))\n",
    "        model.add(Dense(units=100, activation='relu'))\n",
    "        model.add(Dense(units=y_train.shape[-1], activation=None))\n",
    "\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                        loss='mse', metrics=['MAE'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [50, 100, 150]),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    MyHyperModel(),\n",
    "    objective=\"loss\",\n",
    "    max_epochs=50,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"tune_hypermodel_pu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "batch_size = 75\n",
    "epochs = 500\n",
    "\n",
    "## Creating the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=300, activation='relu', input_shape=(X_train.shape[-1],)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=300, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=y_train.shape[-1], activation=None))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping monitorando a Validation Loss\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta = 0.0001, # Valor mínimo de mudança para considerar como melhoria\n",
    "    patience  = 20,     # Quantas épocas esperar até parar\n",
    "    mode = 'auto',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 64 Complete [00h 00m 35s]\n",
      "loss: 3.5475793993100524e-05\n",
      "\n",
      "Best loss So Far: 5.618921932182275e-06\n",
      "Total elapsed time: 00h 15m 10s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train,y=y_train, validation_data=(X_eval,y_eval), epochs=200, callbacks=[early_stopping])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.001\n",
      "batch size 50\n",
      "dropout 0.2\n",
      "units 100\n"
     ]
    }
   ],
   "source": [
    "print('learning rate', best_hps.get('learning_rate'))\n",
    "print('batch size', best_hps.get('batch_size'))\n",
    "print('dropout', best_hps.get('rate'))\n",
    "print('units', best_hps.get('units'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(x=X_train,y=y_train,\n",
    "        validation_data=(X_eval,y_eval),\n",
    "        epochs=300,\n",
    "        callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve MAE curve\n",
    "logs = pd.DataFrame(history.history)\n",
    "\n",
    "# plt.style.use('matplot.mplstyle')\n",
    "plt.figure(figsize = (8,6), dpi = 100)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(logs.loc[1:,\"loss\"], label='training MSE')\n",
    "plt.plot(logs.loc[1:,\"val_loss\"], label='validation MSE')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(logs.loc[1:,\"MAE\"], label='training MAE')\n",
    "plt.plot(logs.loc[1:,\"val_MAE\"], label='validation MAE')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc='upper right')\n",
    "# plt.legend()\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(logs.loc[5:,\"MAE\"], label='training MAE')\n",
    "# plt.plot(logs.loc[5:,\"val_MAE\"], label='validation MAE')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"MAE\")\n",
    "\n",
    "plt.tight_layout()\n",
    "date = datetime.now().strftime('%d_%m-%H_%M')\n",
    "plt.savefig(f\"Loss_Curve_top_phaseA_83-300_{date}.jpg\")\n",
    "plt.savefig(f\"Loss_Curve_top_phaseA_83-300_{date}.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime('%d_%m-%H_%M')\n",
    "model.save(f'PhaseA_top_PU_train_{50}batch_21-300epochs_{date}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n",
      "Mean Absolute Error: 0.421825676526725\n",
      "Mean Squared Error: 0.6519665735218063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(r'PhaseA_top_V_train_50batch_83-300epochs_23_12-15_08.h5')\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test.values,predictions)}\")\n",
    "print(f\"Mean Squared Error: {np.sqrt(mean_squared_error(y_test.values,predictions))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('matplot.mplstyle')\n",
    "# 43 - bar37, 8 - bar9, 2 - bar4, 28 - bar22\n",
    "bar = 8\n",
    "barra = 9\n",
    "plt.figure(figsize = (8,4), dpi = 300)\n",
    "plt.ylabel('Número de Ocorrências', fontsize=16)\n",
    "plt.xlabel('Mean Absolute Error', fontsize=16)\n",
    "plt.title(f'Histograma da barra {barra} para rede em Volts', fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.ylim(0, 450)\n",
    "plt.xlim(0, 0.012)\n",
    "\n",
    "_ = plt.hist(abs(y_test[:][bar].values - predictions[:, bar]), bins=50, density=True, facecolor='g', alpha=0.75, rwidth=0.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"Histograma_barra{barra}_V.jpg\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('matplot.mplstyle')\n",
    "# Tensão em Volts\n",
    "for i in [5,22,65,100,149]:\n",
    "    for bar in [5,15,38]:\n",
    "        t = np.arange(144)\n",
    "        plt.figure(figsize = (13,6), dpi = 300)\n",
    "        plt.plot(t,y_test.values[(i*144):((i*144)+144), bar], 'C1', lw = 3.5, label = 'Dataset')\n",
    "        plt.plot(t,predictions[(i*144):((i*144)+144), bar], 'C0', lw = 2, label = 'Predictions')\n",
    "        \n",
    "        plt.xticks(np.arange(0, 145, 12), ('0h', '2h', '4h', '6h', '8h', '10h', '12h', '14h','16h', '18h', '20h', '22h', '24h'))\n",
    "        plt.ylim(200, 235)\n",
    "        plt.axhline(231, color=\"black\", lw = 2, linestyle=\"--\")\n",
    "        plt.axhline(202, color=\"black\", lw = 2, linestyle=\"--\")\n",
    "        plt.ylabel('V')\n",
    "        plt.title(f'Voltage Bar{bar} - Day {i} - With P/Q as Input', fontsize=22)\n",
    "        plt.legend(loc='upper right', framealpha=1)\n",
    "        plt.savefig(f\"PhaseA_V_Voltage_Bar{bar}_Day{i}_PQ-025.jpg\")\n",
    "        plt.tight_layout()\n",
    "        plt.margins(x=0)\n",
    "        plt.grid(True)\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e91837a7a7811ce1baba58a337e6c2f9326166ec5011331d40c820304c9c4380"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
